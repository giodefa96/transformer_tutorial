{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed Forward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the Attention layer, the Feed Forward Network (Feed Forward or FFN) operates on each token independently of all other tokens in the sequence. It cannot reference other tokens or positional information outside of the information embedded in the current token vector.\n",
    "\n",
    "Formally, the Feed Forward layer is defined as\n",
    "$$\n",
    "FFN=Act(XW^1)W^2\n",
    "$$\n",
    "\n",
    "where $W^1 \\; \\in \\; \\mathbb{R}^{d_{model} \\times d_{FFN}}$​ is a linear layer which projects token vectors into a higher dimensional space d_{FFN}, Act is the activation function ( Most modern Transformers use GeLU, or a Gated Linear Unit (GLU) with GeLU. Google likes to use SiLU and GLU-SiLU. ReLU and Softmax also make appearances from time to time. GLU will be covered in a future post in this series.) , and $W^2 \\; \\in \\; \\mathbb{R}^{d_{model} \\times d_{FFN}}$​ projects the expanded token vectors back down to the input space d_{model}.\n",
    "\n",
    "Note: Act(XW^1) is condenstated expression for two sets of linear equations with a non-linearity in-between\n",
    "$$\n",
    "FNN = Act(XA^1 + B^1)A^2 + B^2\n",
    "$$\n",
    "Where A^1, A^2, B^1 and B^2 are learnable parameters for calculationg the output from input X.\n",
    "\n",
    "The Feed Forward Network can be thought providing an implicit key-value memory to the Transformer layer, with the upscaling projection generating per-token keys into the *FFN’s* working memory. Neurons in the Feed Forward layers are thought to be polysemantic, responding to multiple concepts at once. The superposition hypothesis suggests the neuron’s polysemanticity simulates a much larger layer, which allows the model to understand more features then parameters.\n",
    "\n",
    "The Softmax Linear Units interpretability study by Elhage et al found that early, middle, and late Feed Forward layers likelyElhage et al describe their work as preliminary and requiring more detailed follow-up study. The study also changes the activation function used in most Transformers, which could modify the results. focus on different aspects of language modeling. Early layers are often involved in “detokenizing” inputs into discrete concepts, with neurons which recognize multi-token words, names of famous people, compound words, programming commands, and related nounsRemember that after the first Attention layer, each individual token can be a combination of multiple tokens due to the Attention mechanism. This does not violate the FFN operating on each token individually.. Middle layers tend to respond to abstract ideas. Elhage et al highlight examples such as any clause which describes music, numbers of people, and discourse markers. The final layers of the model tend to focus on converting the discrete concepts back into individual tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In italiano, la spiegazione delle parti che hai fornito sarebbe la seguente:\n",
    "\n",
    "**Rete Feed Forward (FFN)**:\n",
    "A differenza del livello di attenzione, la Rete Feed Forward (FFN) opera su ogni token indipendentemente da tutti gli altri token nella sequenza. Non può fare riferimento ad altri token o informazioni posizionali al di fuori delle informazioni incorporate nel vettore del token corrente.\n",
    "\n",
    "Formalmente, il livello Feed Forward è definito come:\n",
    "$$\n",
    "FFN = Act(XW^1)W^2\n",
    "$$\n",
    "dove $W^1 \\in \\mathbb{R}^{d_{model} \\times d_{FFN}}$ è un livello lineare che proietta i vettori dei token in uno spazio dimensionale superiore $d_{FFN}$, Act è la funzione di attivazione (la maggior parte dei Transformer moderni utilizza GeLU, o una Gated Linear Unit (GLU) con GeLU. Google tende a utilizzare SiLU e GLU-SiLU. ReLU e Softmax compaiono anche di tanto in tanto. GLU sarà trattato in un post futuro di questa serie), e $W^2 \\in \\mathbb{R}^{d_{model} \\times d_{FFN}}$ proietta i vettori dei token espansi di nuovo nello spazio di input $d_{model}$.\n",
    "\n",
    "Nota: $Act(XW^1)$ è un'espressione condensata per due insiemi di equazioni lineari con una non-linearità intermedia:\n",
    "$$\n",
    "FFN = Act(XA^1 + B^1)A^2 + B^2\n",
    "$$\n",
    "Dove $A^1$, $A^2$, $B^1$ e $B^2$ sono parametri apprendibili per calcolare l'output dall'input X.\n",
    "\n",
    "La Rete Feed Forward può essere vista come una memoria chiave-valore implicita per il livello Transformer, con la proiezione di upscaling che genera chiavi per-token nella memoria di lavoro dell'FFN. Si ritiene che i neuroni nei livelli Feed Forward siano polisemantici, rispondendo a più concetti contemporaneamente. L'ipotesi della sovrapposizione suggerisce che la polisemanticità dei neuroni simula un livello molto più grande, il che permette al modello di comprendere più caratteristiche rispetto ai parametri.\n",
    "\n",
    "Lo studio sull'interpretabilità delle Softmax Linear Units condotto da Elhage et al. ha scoperto che i livelli iniziali, intermedi e finali della FFN si concentrano probabilmente su aspetti diversi della modellazione del linguaggio. I livelli iniziali sono spesso coinvolti nella \"detokenizzazione\" degli input in concetti discreti, con neuroni che riconoscono parole composte da più token, nomi di persone famose, parole composte, comandi di programmazione e sostantivi correlati. Ricorda che dopo il primo livello di attenzione, ogni token individuale può essere una combinazione di più token a causa del meccanismo di attenzione. Questo non viola il principio secondo cui la FFN opera su ogni token individualmente. I livelli intermedi tendono a rispondere a idee astratte. Elhage et al. evidenziano esempi come qualsiasi clausola che descrive la musica, il numero di persone e i marcatori discorsivi. I livelli finali del modello tendono a concentrarsi sulla conversione dei concetti discreti di nuovo in token individuali.\n",
    "\n",
    "**Esempio di FFN**:\n",
    "Ecco un semplice esempio di come potrebbe essere implementata una FFN in PyTorch:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ffn):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        # Primo livello lineare\n",
    "        self.linear1 = nn.Linear(d_model, d_ffn)\n",
    "        # Secondo livello lineare\n",
    "        self.linear2 = nn.Linear(d_ffn, d_model)\n",
    "        # Funzione di attivazione (GeLU in questo caso)\n",
    "        self.activation = F.gelu\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Applicazione del primo livello lineare\n",
    "        x = self.linear1(x)\n",
    "        # Applicazione della funzione di attivazione\n",
    "        x = self.activation(x)\n",
    "        # Applicazione del secondo livello lineare\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# Esempio di utilizzo\n",
    "d_model = 512  # Dimensione dei vettori di input\n",
    "d_ffn = 2048   # Dimensione dello spazio proiettato\n",
    "ffn = FeedForwardNetwork(d_model, d_ffn)\n",
    "\n",
    "# Creazione di un input fittizio (batch_size, sequence_length, d_model)\n",
    "input_tensor = torch.rand(64, 10, d_model)\n",
    "\n",
    "# Passaggio dell'input attraverso la FFN\n",
    "output_tensor = ffn(input_tensor)\n",
    "```\n",
    "\n",
    "In questo esempio, abbiamo definito una classe `FeedForwardNetwork` che contiene due livelli lineari e una funzione di attivazione GeLU. L'input passa attraverso il primo livello lineare, viene attivato dalla funzione GeLU e poi passa attraverso il secondo livello lineare per produrre l'output. Questo processo avviene indipendentemente per ogni token nell'input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, `nn.Linear` è una classe che implementa un livello (o strato) lineare, che è anche noto come strato completamente connesso o dense layer in altri framework di deep learning. Questo tipo di strato è uno dei componenti fondamentali delle reti neurali.\n",
    "\n",
    "Un livello lineare trasforma i dati di input applicando una trasformazione lineare, che è essenzialmente una moltiplicazione matriciale seguita dall'aggiunta di un termine di bias. La trasformazione può essere rappresentata dalla seguente equazione:\n",
    "\n",
    "$$\n",
    "y = xW^T + b\n",
    "$$\n",
    "\n",
    "dove:\n",
    "- \\( x \\) è il vettore di input,\n",
    "- \\( W \\) è la matrice dei pesi del livello,\n",
    "- \\( b \\) è il vettore di bias,\n",
    "- \\( y \\) è il vettore di output.\n",
    "\n",
    "La matrice dei pesi \\( W \\) e il vettore di bias \\( b \\) sono i parametri apprendibili del livello, che vengono aggiornati durante il processo di addestramento della rete neurale attraverso la discesa del gradiente e la backpropagation.\n",
    "\n",
    "In PyTorch, puoi creare un livello lineare utilizzando `nn.Linear` specificando il numero di caratteristiche in ingresso e il numero di caratteristiche in uscita:\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "# Numero di caratteristiche in ingresso e in uscita\n",
    "input_features = 5\n",
    "output_features = 3\n",
    "\n",
    "# Creazione di un livello lineare\n",
    "linear_layer = nn.Linear(input_features, output_features)\n",
    "```\n",
    "\n",
    "In questo esempio, `input_features` è la dimensione del vettore di input \\( x \\), e `output_features` è la dimensione del vettore di output \\( y \\). Quando passi un tensore di input attraverso questo livello, PyTorch automaticamente esegue la moltiplicazione matriciale e aggiunge il bias per produrre l'output.\n",
    "\n",
    "I livelli lineari sono usati in molti tipi di reti neurali, inclusi i modelli di classificazione, regressione, reti neurali convoluzionali (CNN) e reti neurali ricorrenti (RNN). Sono particolarmente comuni nelle parti finali di una rete neurale, dove i dati trasformati devono essere mappati in un numero di classi per la classificazione o in valori continui per la regressione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In un contesto di Language Model (LLM), come un Transformer, un livello lineare (o `nn.Linear` in PyTorch) viene utilizzato in diverse parti dell'architettura, e i dati che entrano in questi strati possono variare a seconda della posizione e della funzione dello strato all'interno del modello.\n",
    "\n",
    "Ecco alcuni esempi di come i livelli lineari possono essere utilizzati in un LLM:\n",
    "\n",
    "1. **Embedding dei Token**:\n",
    "   All'inizio del modello, i token di input (di solito sotto forma di indici interi che rappresentano parole o pezzi di parole) vengono trasformati in vettori densi tramite un livello di embedding. Anche se il livello di embedding non è tecnicamente un `nn.Linear`, è concettualmente simile perché associa ogni indice di token a un vettore denso.\n",
    "\n",
    "2. **Proiezione dei Vettori di Embedding**:\n",
    "   Dopo l'embedding, i vettori di embedding possono essere ulteriormente trasformati da un livello lineare. Questo è spesso fatto per adattare la dimensione dell'embedding alla dimensione desiderata del modello (`d_model`).\n",
    "\n",
    "3. **Feed Forward Network (FFN) all'interno di ciascun blocco Transformer**:\n",
    "   Come hai menzionato nella tua domanda precedente, ogni blocco Transformer contiene una FFN che opera su ogni posizione della sequenza indipendentemente. In questo caso, l'input per il livello lineare è il vettore di output del meccanismo di attenzione per ogni posizione della sequenza. Quindi, se la sequenza di input è composta da N token, e ogni token è rappresentato da un vettore di dimensione `d_model`, l'input per il livello lineare della FFN sarà un batch di N vettori di dimensione `d_model`.\n",
    "\n",
    "4. **Output del Modello**:\n",
    "   Alla fine del modello, un livello lineare è spesso utilizzato per trasformare i vettori di output del Transformer (ancora una volta, di dimensione `d_model`) in un punteggio logit per ogni token nel vocabolario. Questi logits sono poi passati attraverso una funzione softmax per ottenere una distribuzione di probabilità su tutti i possibili token successivi.\n",
    "\n",
    "Ecco un esempio di come un token potrebbe essere processato attraverso un livello lineare in un LLM:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Supponiamo che d_model sia 512 e il vocabolario abbia una dimensione di 10000\n",
    "d_model = 512\n",
    "vocab_size = 10000\n",
    "\n",
    "# Creazione di un livello lineare per proiettare l'output del Transformer sui logits del vocabolario\n",
    "output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "# Supponiamo di avere un batch di sequenze con 5 token, e ogni token è rappresentato da un vettore di dimensione 512\n",
    "# (batch_size, sequence_length, d_model)\n",
    "input_tensor = torch.rand(1, 5, d_model)\n",
    "\n",
    "# Passaggio dell'output del Transformer attraverso il livello lineare per ottenere i logits\n",
    "logits = output_layer(input_tensor)\n",
    "\n",
    "# Applicazione della funzione softmax per ottenere le probabilità dei token\n",
    "probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "# Ora, probabilities contiene la distribuzione di probabilità per il prossimo token per ogni posizione della sequenza\n",
    "```\n",
    "\n",
    "In questo esempio, `input_tensor` rappresenta l'output del Transformer per una sequenza di 5 token, e `output_layer` è il livello lineare che trasforma questi vettori in logits, che sono poi convertiti in probabilità. Ogni token nella sequenza viene processato indipendentemente attraverso il livello lineare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficamente, possiamo rappresentare il flusso di dati attraverso un livello lineare in un modello di linguaggio come segue:\n",
    "\n",
    "```\n",
    "Input Tokens:  \"The\", \"quick\", \"brown\", \"fox\"\n",
    "\n",
    "Token IDs:     [ 2,    17,     24,     51 ]\n",
    "\n",
    "Embedding Layer: (trasforma gli ID dei token in vettori densi)\n",
    "+-----+     +------------+\n",
    "|  2  | --> | [0.5, ...] |\n",
    "+-----+     +------------+\n",
    "+-----+     +------------+\n",
    "| 17  | --> | [0.8, ...] |\n",
    "+-----+     +------------+\n",
    "+-----+     +------------+\n",
    "| 24  | --> | [0.3, ...] |\n",
    "+-----+     +------------+\n",
    "+-----+     +------------+\n",
    "| 51  | --> | [0.9, ...] |\n",
    "+-----+     +------------+\n",
    "\n",
    "Trasformazione Lineare (es. FFN all'interno di un blocco Transformer):\n",
    "+------------+     +------------+\n",
    "| [0.5, ...] | --> | [1.2, ...] |\n",
    "+------------+     +------------+\n",
    "+------------+     +------------+\n",
    "| [0.8, ...] | --> | [0.6, ...] |\n",
    "+------------+     +------------+\n",
    "+------------+     +------------+\n",
    "| [0.3, ...] | --> | [0.7, ...] |\n",
    "+------------+     +------------+\n",
    "+------------+     +------------+\n",
    "| [0.9, ...] | --> | [1.1, ...] |\n",
    "+------------+     +------------+\n",
    "\n",
    "Output Layer (trasforma i vettori in logits per ogni token nel vocabolario):\n",
    "+------------+     +-------------------+\n",
    "| [1.2, ...] | --> | [0.01, 0.02, ...] |\n",
    "+------------+     +-------------------+\n",
    "+------------+     +-------------------+\n",
    "| [0.6, ...] | --> | [0.03, 0.04, ...] |\n",
    "+------------+     +-------------------+\n",
    "+------------+     +-------------------+\n",
    "| [0.7, ...] | --> | [0.00, 0.05, ...] |\n",
    "+------------+     +-------------------+\n",
    "+------------+     +-------------------+\n",
    "| [1.1, ...] | --> | [0.06, 0.07, ...] |\n",
    "+------------+     +-------------------+\n",
    "\n",
    "Softmax (converte i logits in probabilità):\n",
    "+-------------------+     +-------------------+\n",
    "| [0.01, 0.02, ...] | --> | [0.001, 0.002, ...] |\n",
    "+-------------------+     +-------------------+\n",
    "+-------------------+     +-------------------+\n",
    "| [0.03, 0.04, ...] | --> | [0.003, 0.004, ...] |\n",
    "+-------------------+     +-------------------+\n",
    "+-------------------+     +-------------------+\n",
    "| [0.00, 0.05, ...] | --> | [0.000, 0.005, ...] |\n",
    "+-------------------+     +-------------------+\n",
    "+-------------------+     +-------------------+\n",
    "| [0.06, 0.07, ...] | --> | [0.006, 0.007, ...] |\n",
    "+-------------------+     +-------------------+\n",
    "```\n",
    "\n",
    "In questa rappresentazione grafica, ogni token della frase di input viene prima convertito in un ID univoco. Questi ID vengono poi trasformati in vettori densi attraverso il livello di embedding. Successivamente, ogni vettore di embedding passa attraverso una trasformazione lineare (ad esempio, una FFN all'interno di un blocco Transformer), che opera su ogni token indipendentemente. Dopo la trasformazione lineare, i vettori risultanti vengono passati attraverso un altro livello lineare (l'output layer) che produce un vettore di logits per ogni token. Infine, i logits vengono convertiti in probabilità attraverso la funzione softmax.\n",
    "\n",
    "Ogni quadrato rappresenta un vettore, e le frecce indicano la trasformazione dei dati da un passaggio all'altro. Nota che in un modello di linguaggio reale, i vettori di embedding e i vettori trasformati avranno dimensioni molto più grandi, e ci saranno molti più token nel vocabolario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nella maggior parte delle implementazioni di reti neurali, compresi i modelli di linguaggio come i Transformer, le trasformazioni lineari sono effettuate su tutti i token contemporaneamente, sfruttando il parallelismo offerto dalle GPU o dalle CPU multicore. Questo è possibile perché le operazioni matematiche sottostanti, come le moltiplicazioni matrice-matrice, sono altamente ottimizzate per essere eseguite in parallelo su batch di dati.\n",
    "\n",
    "Quando si parla di un livello lineare (o fully connected, o dense layer) in una rete neurale, l'input è tipicamente una matrice di dimensione `[batch_size, num_tokens, embedding_size]`, dove:\n",
    "\n",
    "- `batch_size` è il numero di esempi di input che vengono processati insieme in un batch.\n",
    "- `num_tokens` è il numero di token in ogni esempio (che può essere la lunghezza della sequenza dopo il padding).\n",
    "- `embedding_size` è la dimensione dell'embedding di ogni token, che corrisponde anche alla dimensione del modello `d_model` nei Transformer.\n",
    "\n",
    "La trasformazione lineare viene applicata a questa matrice in modo che ogni vettore di embedding di ogni token sia trasformato indipendentemente, ma l'operazione viene eseguita su tutti i token di tutti gli esempi nel batch contemporaneamente. Questo è uno dei motivi per cui l'addestramento delle reti neurali è molto più efficiente su hardware specializzato come le GPU.\n",
    "\n",
    "Per esempio, se abbiamo un batch di dati con 32 esempi (batch_size=32) e ogni esempio è una sequenza di 10 token (num_tokens=10), e ogni token è rappresentato da un vettore di 512 dimensioni (embedding_size=512), l'input al livello lineare sarà una matrice di dimensione `[32, 10, 512]`. Il livello lineare trasformerà questa matrice in una nuova matrice di dimensione `[32, 10, num_output_features]`, dove `num_output_features` è il numero di caratteristiche di output specificato quando si crea il livello lineare.\n",
    "\n",
    "In PyTorch, questo può essere rappresentato come segue:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size = 32\n",
    "num_tokens = 10\n",
    "embedding_size = 512\n",
    "num_output_features = 2048  # per esempio, per una FFN\n",
    "\n",
    "# Creazione di un livello lineare\n",
    "linear_layer = nn.Linear(embedding_size, num_output_features)\n",
    "\n",
    "# Creazione di un input fittizio\n",
    "input_tensor = torch.rand(batch_size, num_tokens, embedding_size)\n",
    "\n",
    "# Applicazione del livello lineare\n",
    "output_tensor = linear_layer(input_tensor)  # La dimensione di output_tensor sarà [32, 10, 2048]\n",
    "```\n",
    "\n",
    "In questo esempio, `output_tensor` conterrà il risultato della trasformazione lineare applicata a ogni token di ogni esempio nel batch, ma l'intera operazione viene eseguita in un solo passaggio, sfruttando il parallelismo dell'hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dopo che la rete neurale ha processato tutti gli elementi di tutti i batch attraverso un livello lineare (o qualsiasi altro livello), il flusso dei dati continua attraverso il resto dell'architettura della rete. Ecco cosa accade tipicamente in un modello di linguaggio come un Transformer:\n",
    "\n",
    "1. **Attivazione Non-Lineare**: Dopo la trasformazione lineare, di solito segue una funzione di attivazione non-lineare. Nel caso dei Transformer, spesso si usa la funzione di attivazione ReLU (Rectified Linear Unit) o GeLU (Gaussian Error Linear Unit). Questo passaggio introduce non-linearità nel modello, che è essenziale per apprendere complesse rappresentazioni dei dati.\n",
    "\n",
    "2. **Eventuali Ulteriori Trasformazioni**: In un blocco Transformer, dopo la FFN e la funzione di attivazione, potrebbe esserci un altro livello lineare che riporta la dimensione dei dati a quella originale del modello (se la FFN ha espanso la dimensione internamente).\n",
    "\n",
    "3. **Normalizzazione e Residual Connection**: I modelli di Transformer utilizzano anche connessioni residue e normalizzazione per facilitare il flusso dei gradienti durante l'addestramento e per stabilizzare l'apprendimento. La connessione residua semplicemente somma l'input del blocco (ad esempio, l'output dell'attenzione multi-testa) all'output del blocco (dopo la FFN), e poi questo risultato viene normalizzato.\n",
    "\n",
    "4. **Passaggio al Blocco Successivo**: Se ci sono più blocchi Transformer nell'architettura, l'output di un blocco diventa l'input del blocco successivo, e il processo si ripete.\n",
    "\n",
    "5. **Output Finale**: Dopo che tutti i blocchi Transformer hanno processato i dati, l'output dell'ultimo blocco viene solitamente trasformato in predizioni finali, come la probabilità di ogni possibile token successivo nella generazione di testo. Questo è ottenuto attraverso un livello lineare finale che mappa l'output del modello alla dimensione del vocabolario, seguito da una funzione softmax per ottenere una distribuzione di probabilità.\n",
    "\n",
    "6. **Calcolo della Perdita e Backpropagation**: Se il modello è in fase di addestramento, l'output finale viene confrontato con il target (le etichette corrette) utilizzando una funzione di perdita (loss function), come la cross-entropy per i compiti di classificazione. Il risultato è una misura di quanto l'output del modello si discosta dal target. Questa perdita viene poi utilizzata per calcolare i gradienti per tutti i pesi della rete attraverso la backpropagation.\n",
    "\n",
    "7. **Aggiornamento dei Pesi**: Utilizzando gli algoritmi di ottimizzazione come SGD (Stochastic Gradient Descent) o varianti più avanzate come Adam, i pesi della rete vengono aggiornati nella direzione che riduce la perdita.\n",
    "\n",
    "8. **Iterazione con Nuovi Batch**: Il processo si ripete con nuovi batch di dati fino a quando il modello non ha completato l'addestramento per il numero di epoche desiderato o fino a quando non soddisfa alcuni criteri di arresto, come una soglia di perdita o un punteggio di validazione.\n",
    "\n",
    "In fase di inferenza (quando il modello è utilizzato per fare previsioni), il processo si ferma dopo l'output finale e la trasformazione softmax, e il risultato viene utilizzato per generare il testo o per altre attività di elaborazione del linguaggio naturale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certamente, posso fornirti una rappresentazione grafica semplificata di una Feed-Forward Network (FFN) all'interno di un blocco Transformer che processa un batch di dati. Consideriamo un batch di dimensione 32, con sequenze di lunghezza 10 e una dimensione di embedding di 512.\n",
    "\n",
    "```\n",
    "+------------------+    +------------------+    +------------------+\n",
    "|                  |    |                  |    |                  |\n",
    "| Input Embeddings |    | Linear Layer 1   |    | Activation (ReLU)|\n",
    "| (Batch, 10, 512) | -> | (Batch, 10, 2048)| -> | (Batch, 10, 2048)|\n",
    "|                  |    |                  |    |                  |\n",
    "+------------------+    +------------------+    +------------------+\n",
    "                             |\n",
    "                             v\n",
    "                      +------------------+\n",
    "                      |                  |\n",
    "                      | Linear Layer 2   |\n",
    "                      | (Batch, 10, 512) |\n",
    "                      |                  |\n",
    "                      +------------------+\n",
    "                             |\n",
    "                             v\n",
    "                      +------------------+\n",
    "                      |                  |\n",
    "                      | Residual & Norm  |\n",
    "                      | (Batch, 10, 512) |\n",
    "                      |                  |\n",
    "                      +------------------+\n",
    "                             |\n",
    "                             v\n",
    "                      +------------------+\n",
    "                      |                  |\n",
    "                      | Output           |\n",
    "                      | (Batch, 10, 512) |\n",
    "                      |                  |\n",
    "                      +------------------+\n",
    "```\n",
    "\n",
    "In questa rappresentazione:\n",
    "\n",
    "1. **Input Embeddings**: I dati di input sono rappresentati da una matrice tridimensionale con dimensioni `(Batch, 10, 512)`, dove \"Batch\" è la dimensione del batch (32 in questo caso), 10 è il numero di token per sequenza, e 512 è la dimensione dell'embedding di ogni token.\n",
    "\n",
    "2. **Linear Layer 1**: Il primo livello lineare trasforma l'embedding di dimensione 512 in una dimensione intermedia più grande, diciamo 2048 per questo esempio. Questo passaggio è rappresentato dalla freccia che porta a una nuova matrice di dimensioni `(Batch, 10, 2048)`.\n",
    "\n",
    "3. **Activation (ReLU)**: Una funzione di attivazione non lineare, come ReLU, viene applicata a ogni elemento della matrice risultante dal primo livello lineare.\n",
    "\n",
    "4. **Linear Layer 2**: Un secondo livello lineare riduce la dimensione dai 2048 elementi intermedi di nuovo a 512, la dimensione originale dell'embedding.\n",
    "\n",
    "5. **Residual & Norm**: Viene aggiunta la connessione residua (l'input originale dell'FFN) all'output del secondo livello lineare, e poi viene applicata la normalizzazione per stabilizzare l'apprendimento.\n",
    "\n",
    "6. **Output**: L'output finale dell'FFN ha la stessa dimensione dell'input, `(Batch, 10, 512)`, e può essere passato al blocco Transformer successivo o utilizzato per ulteriori elaborazioni.\n",
    "\n",
    "Questa rappresentazione è una semplificazione ad alto livello e non mostra dettagli come i bias che vengono aggiunti in ogni livello lineare o le specifiche della normalizzazione. Tuttavia, fornisce una visione generale di come i dati fluiscono attraverso una FFN in un blocco Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certo, a livello matematico, un livello lineare (anche noto come fully connected o dense layer) in una rete neurale esegue una trasformazione affine sui dati di input. Questa trasformazione è definita dalla seguente equazione:\n",
    "$$\n",
    " \\textbf{y} = \\textbf{Wx} + \\textbf{b} \n",
    "$$\n",
    "Dove:\n",
    "- $\\textbf{x}$ è il vettore di input (o la matrice di input nel caso di batch di dati).\n",
    "- $\\textbf{W}$ è la matrice dei pesi del livello lineare.\n",
    "- $\\textbf{b}$ è il vettore di bias.\n",
    "- $\\textbf{y}$ è il vettore di output (o la matrice di output nel caso di batch di dati).\n",
    "\n",
    "Nel contesto di un livello lineare all'interno di una FFN di un Transformer, consideriamo un batch di input con dimensione `(Batch, 10, 512)`. Per ogni token nella sequenza (10 in totale per ogni esempio nel batch), il livello lineare trasforma il vettore di input di dimensione 512 in un nuovo vettore di dimensione 2048 (o qualsiasi altra dimensione impostata per il livello).\n",
    "\n",
    "Se rappresentiamo l'input e l'output come matrici, dove ogni riga corrisponde a un token e ogni colonna a una dimensione dell'embedding, la trasformazione per un singolo token può essere vista come:\n",
    "$$\n",
    "\\textbf{y}_{i} = \\textbf{W}\\textbf{x}_{i} + \\textbf{b} \n",
    "$$\n",
    "Dove:\n",
    "$$\n",
    "- \\textbf{x}_{i} è il vettore di input per il token i-esimo, di dimensione 512.\n",
    "- \\textbf{y}_{i} è il vettore di output per il token i-esimo, di dimensione 2048.\n",
    "- i varia da 1 a 10 per ogni esempio nel batch.\n",
    "$$\n",
    "Per l'intero batch, questa operazione viene eseguita in parallelo per tutti i token di tutti gli esempi nel batch, risultando in una matrice di output di dimensione `(Batch, 10, 2048)`.\n",
    "\n",
    "In termini di implementazione, ad esempio in PyTorch, il livello lineare è rappresentato dalla classe `nn.Linear`, che inizializza internamente la matrice dei pesi $\\textbf{W}$ e il vettore di bias $\\textbf{b}$ e fornisce un metodo per eseguire la trasformazione affine sui dati di input.\n",
    "\n",
    "Ecco un esempio di codice che mostra come potrebbe essere implementato in PyTorch:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Dimensioni\n",
    "batch_size = 32\n",
    "sequence_length = 10\n",
    "input_size = 512\n",
    "output_size = 2048\n",
    "\n",
    "# Creazione del livello lineare\n",
    "linear_layer = nn.Linear(input_size, output_size)\n",
    "\n",
    "# Input tensor di dimensione (batch_size, sequence_length, input_size)\n",
    "input_tensor = torch.randn(batch_size, sequence_length, input_size)\n",
    "\n",
    "# Applicazione del livello lineare\n",
    "output_tensor = linear_layer(input_tensor)  # Output di dimensione (batch_size, sequence_length, output_size)b\n",
    "```\n",
    "\n",
    "In questo esempio, `input_tensor` rappresenta il batch di sequenze di token, e `output_tensor` è il risultato della trasformazione lineare applicata a ogni token di ogni sequenza nel batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice dei pesi $\\textbf{W} $ in un livello lineare che trasforma un vettore di input di dimensione 512 in un vettore di output di dimensione 2048 ha la forma:\n",
    "$$\n",
    " \\textbf{W} : (2048 \\times 512) \n",
    "$$\n",
    "Ogni riga della matrice \\( \\textbf{W} \\) corrisponde a una delle 2048 caratteristiche di output, e ogni colonna corrisponde a una delle 512 caratteristiche di input. Quando moltiplichiamo la matrice \\( \\textbf{W} \\) per un vettore di input \\( \\textbf{x} \\) di dimensione 512, otteniamo un vettore di output \\( \\textbf{y} \\) di dimensione 2048.\n",
    "\n",
    "La moltiplicazione matrice-vettore avviene come segue:\n",
    "$$\n",
    " \\textbf{y} = \\textbf{Wx} \n",
    "$$\n",
    "Dove:\n",
    "- $\\textbf{x}$ è un vettore colonna di dimensione $ (512 \\times 1) $.\n",
    "-  $\\textbf{W}$ è la matrice dei pesi di dimensione  $(2048 \\times 512) $.\n",
    "- $\\textbf{y}$ è il vettore risultante di dimensione $ (2048 \\times 1)$.\n",
    "\n",
    "In un contesto di batch processing, dove abbiamo un batch di vettori di input, la moltiplicazione diventa una moltiplicazione matrice-matrice. Se il batch di input è una matrice $ \\textbf{X}$ di dimensione $(Batch, 10, 512)$, allora per ogni token in ogni esempio del batch, la trasformazione lineare è applicata come segue:\n",
    "$$\n",
    "\\textbf{Y}_{i} = \\textbf{W}\\textbf{X}_{i} + \\textbf{b} \n",
    "$$\n",
    "Dove:\n",
    "- $\\textbf{X}_{i} $ è la matrice di input per il token $i$-esimo di ogni esempio nel batch, di dimensione $(Batch, 512)$.\n",
    "- $\\textbf{Y}_{i}$ è la matrice di output per il token $i$-esimo di ogni esempio nel batch, di dimensione $(Batch, 2048)$.\n",
    "-  $\\textbf{b}$  è il vettore di bias che viene sommato a ciascun vettore di output, di dimensione $ (2048) $, e viene esteso (broadcasted) per corrispondere alle dimensioni del batch.\n",
    "\n",
    "In pratica, le librerie di deep learning come PyTorch e TensorFlow gestiscono automaticamente queste operazioni di moltiplicazione e broadcasting, permettendo di lavorare con batch di dati in modo efficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gdefeudis\\Code\\transformer_tutorial\\.venv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Dimensioni\n",
    "batch_size = 32\n",
    "sequence_length = 10\n",
    "input_size = 512\n",
    "output_size = 2048\n",
    "\n",
    "# Creazione del livello lineare\n",
    "linear_layer = nn.Linear(input_size, output_size)\n",
    "\n",
    "# Input tensor di dimensione (batch_size, sequence_length, input_size)\n",
    "input_tensor = torch.randn(batch_size, sequence_length, input_size)\n",
    "\n",
    "# Applicazione del livello lineare\n",
    "output_tensor = linear_layer(input_tensor)  # Output di dimensione (batch_size, sequence_length, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 2048])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
